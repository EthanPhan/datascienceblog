<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ethan's (sort of) Data Science Blog</title><link>https://ethanphan.github.io/datascienceblog/</link><description>A blog about data science and AI stuff</description><atom:link href="https://ethanphan.github.io/datascienceblog/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2019 &lt;a href="mailto:longfet53@gmail.com"&gt;Ethan Phan&lt;/a&gt; </copyright><lastBuildDate>Wed, 10 Jul 2019 04:17:59 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Linear Discriminant Analysis</title><link>https://ethanphan.github.io/datascienceblog/posts/linear-discriminant-analysis/</link><dc:creator>Ethan Phan</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;font color="#CD5C5C"&gt;Linear Discriminant Analysis(LDA) is a very common technique used for supervised classification problems. Let's dig in to understand what is it, how it works, how we should use it.&lt;/font&gt;&lt;/p&gt;
&lt;p&gt;&lt;a data-flickr-embed="true" href="https://www.flickr.com/photos/15609463@N03/14898932531" title="Reading between the lines"&gt;&lt;img src="https://live.staticflickr.com/5574/14898932531_0935b80b98_h.jpg" width="800" height="500" alt="Reading between the lines"&gt;&lt;/a&gt;&lt;script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"&gt;&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ethanphan.github.io/datascienceblog/posts/linear-discriminant-analysis/"&gt;Read more…&lt;/a&gt; (5 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><guid>https://ethanphan.github.io/datascienceblog/posts/linear-discriminant-analysis/</guid><pubDate>Mon, 08 Jul 2019 14:05:59 GMT</pubDate></item><item><title>My note on Ordinary Least Squares</title><link>https://ethanphan.github.io/datascienceblog/posts/my-note-on-ordinary-least-squares/</link><dc:creator>Ethan Phan</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;font color="#CD5C5C"&gt;In statistics, ordinary least squares (OLS) is a type of linear least squares method for estimating the unknown parameters in a linear regression model. OLS chooses the parameters of a linear function of a set of explanatory variables by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being predicted) in the given dataset and those predicted by the linear function.&lt;/font&gt;&lt;/p&gt;
&lt;h3&gt;&lt;font color="#F08080"&gt;The True Model&lt;/font&gt;&lt;/h3&gt;
&lt;p&gt;Suppose the data consists of N observations $\{x_i, y_i\}_{i=1}^{N}$ . Each observation i includes a scalar response $y_i$ and a column vector $x_i$ of values of K predictors (regressors) $x_{ij}$ for j = 1, ..., K. In a linear regression model, the response variable, $y_i$ is a linear function of the regressors:&lt;/p&gt;
$$ y_i = \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_P x_{iK} + \epsilon_i, $$&lt;p&gt;or in vector form,&lt;/p&gt;
$$ y_i = x_i^T \beta + \epsilon_i, $$&lt;p&gt;where $\beta$ is a K×1 vector of unknown parameters; the $\epsilon_i$'s are unobserved scalar random variables (errors) which account for influences upon the responses $y_i$ from sources other than the explanators $x_i$; and $x_{i}$ is a column vector of the ith observations of all the explanatory variables. This model can also be written in matrix notation as&lt;/p&gt;
$$ y = X \beta + \epsilon \qquad (1)$$&lt;p&gt;&lt;a href="https://ethanphan.github.io/datascienceblog/posts/my-note-on-ordinary-least-squares/"&gt;Read more…&lt;/a&gt; (10 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><guid>https://ethanphan.github.io/datascienceblog/posts/my-note-on-ordinary-least-squares/</guid><pubDate>Thu, 27 Jun 2019 14:31:16 GMT</pubDate></item><item><title>Maximum likelihood estimators and least squares</title><link>https://ethanphan.github.io/datascienceblog/posts/maximum-likelihood-estimators-and-least-squares/</link><dc:creator>Ethan Phan</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Why do we choose to minimize &lt;strong&gt;&lt;em&gt;Mean Square Error&lt;/em&gt;&lt;/strong&gt; (least square) when we do linear regression? Is it because it is smooth and easy to solve its direvative? Is it because that's out intuition about how to fit a curve to a set of points? As it turn out, there is a mathematical reason behind this and It has something to do with &lt;strong&gt;&lt;em&gt;Maximum Likelihood&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;&lt;font color="#F08080"&gt;Maximum likelihood estimators (MLE)&lt;/font&gt;&lt;/h3&gt;
&lt;p&gt;A maximum likelihood estimate for some hidden parameter $\lambda$ (or parameters, plural) of some probability distribution is a number $\hat{\lambda}$ computed from an independent identical distribution (i.i.d.) sample $X_{1} , ..., X_{n}$ from the given distribution that maximizes something called the “likelihood function”. Suppose that the distribution in question is governed by a pdf $f(x; \lambda_{1}, ..., \lambda_{k})$, where the $\lambda_{i}$’s are all hidden parameters. The likelihood function associated to the sample is just&lt;/p&gt;
$$ L(X_{1}, X_{2}, ..., X_{n}) = \prod_{1}^{n}f(X_{i}; \lambda_{1}, ..., \lambda_{k}) $$&lt;p&gt;&lt;a href="https://ethanphan.github.io/datascienceblog/posts/maximum-likelihood-estimators-and-least-squares/"&gt;Read more…&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><guid>https://ethanphan.github.io/datascienceblog/posts/maximum-likelihood-estimators-and-least-squares/</guid><pubDate>Fri, 14 Jun 2019 15:30:07 GMT</pubDate></item><item><title>MSE and Bias-Variance decomposition</title><link>https://ethanphan.github.io/datascienceblog/posts/mse-and-bias-variance-decomposition/</link><dc:creator>Ethan Phan</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;I was reading the book "The Elements of Statistical Learning The Elements of Statistical Learning" to the part about &lt;em&gt;MSE&lt;/em&gt; (mean square error) and  &lt;em&gt;bias–variance decomposition&lt;/em&gt; and it's confusing to me. Understand this is very important to be able to have a good grasp of underfitting, overfitting. Unfortunately, The book didn't explain it clearly (or I was just too stupid for the book). So, I sought the explain on the internet and I found one. Here I will write it down for future reference. There are two common contexts: MSE for estimator and MSE for predictor.&lt;/p&gt;
&lt;p&gt;Wait, WTF is an estimator and a predictor?&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;"Prediction" and "estimation" indeed are sometimes used interchangeably in non-technical writing and they seem to function similarly, but there is a sharp distinction between them in the standard model of a statistical problem. An estimator uses data to guess at a parameter while a predictor uses the data to guess at some random value that is not part of the dataset.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="MSE-for-estimator"&gt;MSE for estimator&lt;a class="anchor-link" href="https://ethanphan.github.io/datascienceblog/posts/mse-and-bias-variance-decomposition/#MSE-for-estimator"&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;em&gt;Estimator&lt;/em&gt; is any function on a sample of the data that usually tries to estimate some useful qualities of the original data from which the sample is drawn. Formally, estimator is a function on a sample S:
$$ \hat{\theta}_{S}=g(S), S=(x_{1}, x_{2},..., x_{m}) $$
where $x_{i}$ is a random variable drawn from a unknown distribution $D$. i.e. $x_{i} \sim D$
&lt;/p&gt;&lt;p&gt;&lt;a href="https://ethanphan.github.io/datascienceblog/posts/mse-and-bias-variance-decomposition/"&gt;Read more…&lt;/a&gt; (9 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><guid>https://ethanphan.github.io/datascienceblog/posts/mse-and-bias-variance-decomposition/</guid><pubDate>Wed, 12 Jun 2019 14:48:39 GMT</pubDate></item><item><title>Bayes Boundary with Multivariate Mixture Gaussian Distributions</title><link>https://ethanphan.github.io/datascienceblog/posts/bayes-boundary-with-multivariate-mixture-gaussian-distributions/</link><dc:creator>Ethan Phan</dc:creator><description>&lt;div&gt;&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Multivariate-Mixture-Gaussian-Model"&gt;Multivariate Mixture Gaussian Model&lt;a class="anchor-link" href="https://ethanphan.github.io/datascienceblog/posts/bayes-boundary-with-multivariate-mixture-gaussian-distributions/#Multivariate-Mixture-Gaussian-Model"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;h3 id="Problem-statement:"&gt;Problem statement:&lt;a class="anchor-link" href="https://ethanphan.github.io/datascienceblog/posts/bayes-boundary-with-multivariate-mixture-gaussian-distributions/#Problem-statement:"&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Create a data set with N = 500 points from two mixed Gaussian distributions (each distribution has five bivariate Gaussian distributions). The elements of the first mixed distribution have a maximum average value of 0 and a minimum average of -5 and a variance of 1. The elements of the second mixed distribution have a maximum mean value of 5, the minimum average is 0 and the variance is 1. Draw decision boundary (Bayes boundary) between N points of the first mixture distribution and N points of the second mixture distribution &lt;strong&gt;without using any machine learning models&lt;/strong&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [1]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# import things we might need&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy.random&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h4 id="Generate-samples"&gt;Generate samples&lt;a class="anchor-link" href="https://ethanphan.github.io/datascienceblog/posts/bayes-boundary-with-multivariate-mixture-gaussian-distributions/#Generate-samples"&gt;¶&lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;ul&gt;
&lt;li&gt;We assume the distribution of mean of the 5 gausian distributions is uniform&lt;/li&gt;
&lt;li&gt;For simplicity, we assume each variable of a bivariate distribution is independent of each other =&amp;gt; covariance matrix is diagonal matrix [[1, 0], [0, 1]]&lt;p&gt;&lt;a href="https://ethanphan.github.io/datascienceblog/posts/bayes-boundary-with-multivariate-mixture-gaussian-distributions/"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><guid>https://ethanphan.github.io/datascienceblog/posts/bayes-boundary-with-multivariate-mixture-gaussian-distributions/</guid><pubDate>Mon, 10 Jun 2019 01:17:46 GMT</pubDate></item></channel></rss>