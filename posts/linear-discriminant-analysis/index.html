<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Linear Discriminant Analysis | Ethan's (sort of) Data Science Blog</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../../rss.xml">
<link rel="canonical" href="https://ethanphan.github.io/datascienceblog/posts/linear-discriminant-analysis/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/latest/css/font-awesome.min.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css">
<meta name="author" content="Ethan Phan">
<link rel="prev" href="../my-note-on-ordinary-least-squares/" title="My note on Ordinary Least Squares" type="text/html">
<link rel="next" href="../the-effect-of-data-shuffling-in-mini-batch-training/" title="The effect of data shuffling in mini-batch training" type="text/html">
<meta property="og:site_name" content="Ethan's (sort of) Data Science Blog">
<meta property="og:title" content="Linear Discriminant Analysis">
<meta property="og:url" content="https://ethanphan.github.io/datascienceblog/posts/linear-discriminant-analysis/">
<meta property="og:description" content="Linear Discriminant Analysis(LDA) is a very common technique used for supervised classification problems. Let's dig in to understand what is it, how it works, how we should use it.



Table of Content">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2019-07-08T22:05:59+08:00">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" integrity="sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv+uR2SE/mbQcG7ATL28aI9H0" crossorigin="anonymous">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ethanphan.github.io/datascienceblog/">

                <span id="blog-title">Ethan's (sort of) Data Science Blog</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="../../archive.html">Archives</a>
                </li>
<li>
<a href="../../categories/">Tags</a>
                </li>
<li>
<a href="../../rss.xml">RSS feed</a>

                
            </li>
</ul>
<!-- Google custom search --><form method="get" action="https://www.google.com/search" class="navbar-form navbar-right" role="search">
<div class="form-group">
<input type="text" name="q" class="form-control" placeholder="Search">
</div>
<button type="submit" class="btn btn-primary">
      <span class="glyphicon glyphicon-search"></span>
</button>
<input type="hidden" name="sitesearch" value="https://ethanphan.github.io/datascienceblog/">
</form>
<!-- End of custom search -->


            <ul class="nav navbar-nav navbar-right">
<li>
    <a href="index.ipynb" id="sourcelink">Source</a>
    </li>

                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Linear Discriminant Analysis</a></h1>

        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                    Ethan Phan
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2019-07-08T22:05:59+08:00" itemprop="datePublished" title="2019-07-08 22:05">2019-07-08 22:05</time></a>
            </p>
            
        <p class="sourceline"><a href="index.ipynb" class="sourcelink">Source</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><font color="#CD5C5C">Linear Discriminant Analysis(LDA) is a very common technique used for supervised classification problems. Let's dig in to understand what is it, how it works, how we should use it.</font></p>
<p><a data-flickr-embed="true" href="https://www.flickr.com/photos/15609463@N03/14898932531" title="Reading between the lines"><img src="https://live.staticflickr.com/5574/14898932531_0935b80b98_h.jpg" width="800" height="500" alt="Reading between the lines"></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script></p>
<!-- TEASER_END -->

<p><strong>Table of Contents</strong></p>
<ul>
<li><a href="#what_is_lda">What is Linear Discriminant Analysis ?</a></li>
<li>
<a href="#howitwork">How does Linear Discriminant Analysis Work ?</a><ul>
<li><a href="#fisher_linear_discriminant">Fisher's linear discriminant</a></li>
<li><a href="#multiclass_lda">Multiple Discriminant Analysis (MDA)</a></li>
</ul>
</li>
<li><a href="#classification_with_lda">Classification with Linear Discriminant Analysis</a></li>
</ul>
<h3><font color="#F08080">What is Linear Discriminant Analysis ?<a class="anchor" id="what_is_lda"></a></font></h3>
<p><a href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis">Wikipedia: </a><strong>Linear discriminant analysis (LDA)</strong>, <strong>normal discriminant analysis (NDA)</strong>, or <strong>discriminant function analysis</strong> is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.</p>
<p>The main goal of dimensionality reduction is to remove redundant or dependent features by transforming features to a lower dimension space. So, what is the difference between LDA and PCA? LDA is a supervised learning technique while PCA is usupervised.</p>
<p><strong>The objective of LDA is to perform dimensionality reduction while preserving as much of the class disciminity information as possible.</strong></p>
<h3><font color="#F08080">How does Linear Discriminant Analysis Work ?<a class="anchor" id="howitwork"></a></font></h3>
<h4 id="Fisher's-linear-discriminant-(2-classes)-">Fisher's linear discriminant (2 classes) <a class="anchor" id="fisher_linear_discriminant"></a><a class="anchor-link" href="#Fisher's-linear-discriminant-(2-classes)-">¶</a>
</h4>
<p>Assume we have a set of D-dimensional sample $\{x_1, x_2, ... , x_N \}$ $N_1$ of which belong to class $c_1$ and $N_2$ of which belong to class  $c_2$. We seek to obtain a scalar $y$ by projecting samples $x$ onto a line</p>
$$ y = w^Tx \qquad (1)$$<p>We would like to choose the line that maximize the separability of the scalars. <strong>To do that, we need to define a metric to measure the separation of the projections.</strong></p>
<p>Suppose two classes of observations have means $\vec{\mu}_1$, $\vec{\mu}_2$ and covariances $\Sigma _{1}$, $\Sigma _{2}$. Then mean and variance of the projections of the two class are $w^T\vec{\mu}_i$ and $w^T \Sigma _{i} w$ for i = 0, 1.</p>
<p>We could choose the distance between the means of the classes as our objective function</p>
$$ J(w) = |w^T\vec{\mu}_1 - w^T\vec{\mu}_2| = |w^T(\vec{\mu}_1 - \vec{\mu}_2)| \qquad (2)$$<p>However, distance between the projected means might not be a good measure as you can see here</p>
<p><img src="https://i.imgur.com/Rq03xZQ.png" alt=""></p>
<p>Fisher defined the separation between these two distributions to be the ratio of the variance between the classes to the variance within the classes:</p>
$$ J(w) = \frac{\sigma_{between}}{\sigma_{within}} = \frac{(w^T(\vec{\mu}_1 - \vec{\mu}_2))^2}{w^T \Sigma _1 w + w^T \Sigma _2 w} = \frac{(w^T(\vec{\mu}_1 - \vec{\mu}_2))^2}{w^T (\Sigma _1 + \Sigma _2) w} = \frac{w^T S_B w}{w^TS_W w} \qquad (3) $$<p>where</p>
$$ S_B = (\vec{\mu}_1 - \vec{\mu}_2)(\vec{\mu}_1 - \vec{\mu}_2)^T $$$$ S_W = \Sigma _1 + \Sigma _2 $$<p>In some sense, this measure is the signal to noise ratio of class labeling. To find the optimum, we can set the derivative of J and set it to zero</p>
$$ \frac{dJ(w)}{dw} = 0 \qquad (4)$$$$ \frac{d[(w^TS_B w)](w^T S_W w) - d[w^T S_W w] (w^T S_B w)}{(w^T S_W w)^2} = 0 \qquad (5)$$$$ d[(w^TS_B w)] (w^T S_W w)  - d[w^T S_W w](w^T S_B w) = 0 \qquad (6)$$$$ 2 S_B w(w^T S_W w) - 2S_W w(w^T S_B w) = 0 \qquad (7)$$<p>Since both $w^T S_W w$ and $w^T S_B w$ are scalars
$$ w^T S_W w (S_B w) - w^T S_B w (S_W w) = 0 \qquad (8)$$</p>
$$ \frac{w^T S_W w (S_B w)}{w^T S_W w} - \frac{w^T S_B w (S_W w)}{w^T S_W w} = 0 \qquad (9) $$$$ S_B w = \lambda S_W w \qquad (10)$$<p>where $\lambda = \frac{w^T S_B w }{w^T S_W w}$ is a scalar depends on $w$
If $S_W$ has full rank, Eq. 10 can be written as</p>
$$ S^{-1}_W S_B w = \lambda w \qquad (11) $$<p>Eq. 11 is a standard eigenvalue problem.</p>
<p>Note that $S_B x$ for any vector $x$, points in the same direction as $\vec{\mu}_1 - \vec{\mu}_2$</p>
$$ S_B x = (\vec{\mu}_1 - \vec{\mu}_2) (\vec{\mu}_1 - \vec{\mu}_2)^T x =  (\vec{\mu}_1 - \vec{\mu}_2) \alpha \qquad (12)$$<p>Thus Eq. 11 can be solve with</p>
$$ \boxed{w = S^{-1}_W (\vec{\mu}_1 - \vec{\mu}_2)} \qquad (13) $$$$ S^{-1}_W S_B w = S^{-1}_W S_B [S^{-1}_W (\vec{\mu}_1 - \vec{\mu}_2)] = S^{-1}_W \alpha (\vec{\mu}_1 - \vec{\mu}_2) = \alpha [S^{-1}_W (\vec{\mu}_1 - \vec{\mu}_2)] = \alpha w \qquad (14)$$<p>We can easily prove that $\alpha = \lambda = \frac{w^T S_B w }{w^T S_W w}$ Do note that the roles of $\vec{\mu}_1$ and $\vec{\mu}_2$ are interchangeable.</p>
<p>The Eq. 13 is often known as <strong>Fisher's Linear Discriminant</strong> (1936), although it is not a dicriminant but rather a specific choice of direction for the projection od data down to one dimension.</p>
<p>Be sure to note that the vector $w$ is the normal to the discriminant hyperplane. As an example, in a two dimensional problem, the line that best divides the two groups is perpendicular to $w$.</p>
<h4 id="Multiple-Discriminant-Analysis-(MDA)-">Multiple Discriminant Analysis (MDA) <a class="anchor" id="multiclass_lda"></a><a class="anchor-link" href="#Multiple-Discriminant-Analysis-(MDA)-">¶</a>
</h4>
<p>In the case where there are more than two classes, the analysis used in the derivation of the Fisher discriminant can be extended to find a subspace which appears to contain all of the class variability.</p>
<p>We seek to obtain projection of sample to a linear subspace: $ y = V^T x$. $V$ is call <strong>projection matrix</strong>.</p>
<p>Let $n_i$ be the number of samples of class $C_i$; $\mu_i$ be the sample mean of class $C_i$; and $\mu$ be the total mean of all samples</p>
$$ \mu_i = \frac{1}{n_i} \sum_{x \in C_i} x \qquad \mu = \frac{1}{n} \sum_{x_i} x_i $$<p>The scatter within class variability may be defined by the total sum of covariance of each class</p>
$$ S_W = \sum_{i=1}^{c}S_i = \sum_{i=1}^{c} \sum_{x_k \in C_i} (x_k - \mu_i)(x_k - \mu_i)^t \qquad (15)$$<p>The scatter between class variability may be defined by the sample covariance of the class means</p>
$$ S_B = \sum_{i=1}^{c} n_i (\mu_i - \mu)(\mu_i - \mu)^T \qquad(16) $$<p>The class seperation for a projection $V$ in this case will be given by</p>
$$ J(V) = \frac{det(V^T \Sigma_b V)}{det(V^T \Sigma V)} \qquad (17) $$<p>To maximize $J(V)$ we first need to solve the <strong>generalized eigenvalue</strong> problem:</p>
$$ S_Bv = \lambda S_W v \qquad (18)$$<p>Note that matrix $S_B$ is of rank $c-1$ at most, Eq. 18 has atmost $c-1$ distinct solution eigenvalues $\lambda$. Let $v_1, v_2 ..., v_{c-1}$ be the corresponding eigenvectors. The optimal projection matrix V to a subspace of dimension $k$ is given by the eigenvectors corresponding to the largest $k$ eigenvalues.</p>
<h3><font color="#F08080">Classification with Linear Discriminant Analysis<a class="anchor" id="classification_with_lda"></a></font></h3>
<p>So far, LDA has been described as a mean of dimensionality reduction to reduce computational cost and avoid over-fitting. After applying this dimensionality reduction, classification can be performed using Bayes' rule approach. This section will be updated in the future.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
</div>
    </div>
    <aside class="postpromonav"><nav><ul class="pager hidden-print">
<li class="previous">
                <a href="../my-note-on-ordinary-least-squares/" rel="prev" title="My note on Ordinary Least Squares">Previous post</a>
            </li>
            <li class="next">
                <a href="../the-effect-of-data-shuffling-in-mini-batch-training/" rel="next" title="The effect of data shuffling in mini-batch training">Next post</a>
            </li>
        </ul></nav></aside><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js" integrity="sha384-/y1Nn9+QQAipbNQWU65krzJralCnuOasHncUFXGkdwntGeSvQicrYkiUBwsgUqc1" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js" integrity="sha256-ExtbCSBuYA7kq1Pz362ibde9nnsHYPt6JxuxYeZbU+c=" crossorigin="anonymous"></script><script>
                renderMathInElement(document.body,
                    {
                        
delimiters: [
    {left: "$$", right: "$$", display: true},
    {left: "\\[", right: "\\]", display: true},
    {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
    {left: "$", right: "$", display: false},
    {left: "\\(", right: "\\)", display: false}
]

                    }
                );
            </script></article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2019         <a href="mailto:longfet53@gmail.com">Ethan Phan</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
        </footer>
</div>
</div>


            <script src="../../assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script>
</body>
</html>
