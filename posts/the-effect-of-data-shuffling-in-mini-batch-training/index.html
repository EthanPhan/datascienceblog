<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>The effect of data shuffling in mini-batch training | Ethan's (sort of) Data Science Blog</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../../rss.xml">
<link rel="canonical" href="https://ethanphan.github.io/datascienceblog/posts/the-effect-of-data-shuffling-in-mini-batch-training/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/latest/css/font-awesome.min.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css">
<meta name="author" content="Ethan Phan">
<link rel="prev" href="../linear-discriminant-analysis/" title="Linear Discriminant Analysis" type="text/html">
<meta property="og:site_name" content="Ethan's (sort of) Data Science Blog">
<meta property="og:title" content="The effect of data shuffling in mini-batch training">
<meta property="og:url" content="https://ethanphan.github.io/datascienceblog/posts/the-effect-of-data-shuffling-in-mini-batch-training/">
<meta property="og:description" content="Often when we train a neural network with mini batches we shuffle the training set before every epoch. It is a very good practice but why? Do we need to do this? I'll try to answer these question in t">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2019-07-25T18:22:31+08:00">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" integrity="sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv+uR2SE/mbQcG7ATL28aI9H0" crossorigin="anonymous">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ethanphan.github.io/datascienceblog/">

                <span id="blog-title">Ethan's (sort of) Data Science Blog</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="../../archive.html">Archives</a>
                </li>
<li>
<a href="../../categories/">Tags</a>
                </li>
<li>
<a href="../../rss.xml">RSS feed</a>

                
            </li>
</ul>
<!-- Google custom search --><form method="get" action="https://www.google.com/search" class="navbar-form navbar-right" role="search">
<div class="form-group">
<input type="text" name="q" class="form-control" placeholder="Search">
</div>
<button type="submit" class="btn btn-primary">
      <span class="glyphicon glyphicon-search"></span>
</button>
<input type="hidden" name="sitesearch" value="https://ethanphan.github.io/datascienceblog/">
</form>
<!-- End of custom search -->


            <ul class="nav navbar-nav navbar-right">
<li>
    <a href="index.ipynb" id="sourcelink">Source</a>
    </li>

                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">The effect of data shuffling in mini-batch training</a></h1>

        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                    Ethan Phan
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2019-07-25T18:22:31+08:00" itemprop="datePublished" title="2019-07-25 18:22">2019-07-25 18:22</time></a>
            </p>
            
        <p class="sourceline"><a href="index.ipynb" class="sourcelink">Source</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><font color="#CD5C5C">Often when we train a neural network with mini batches we shuffle the training set before every epoch. It is a very good practice but why? Do we need to do this? I'll try to answer these question in this blog post.</font></p>
<p><img src="https://i.vimeocdn.com/video/770139883.jpg" alt="data shuffling"></p>
<!-- TEASER_END -->


<p>When we train a neural network, we try to minimize an objective function, which is</p>
$$J(\theta) = \frac{1}{N}\sum_{i=1}^{N}L(\theta, x_i) \qquad (1)$$<p>where $N$ is the number of samples in the training set and $x_i$ is the $ith$ training sample. This objective is a function of the set of parameters $\theta$ of the model and is parameterized by the whole training set. This is only practical when our training set is small. When the dataset becomes larger, the amount of time needed for one update can become too large. This is where mini-batch gradient descent comes to the rescue. Mini-batch gradient descent make the model update frequency higher than batch gradient descent which speed up the training process.</p>
<p>But mini-batch training has its own shortcomings. When we use mini-batch training, we are not minimizing the objective function defined in Eq. 1. We actually minimizes a sequence of approximate functions</p>
$$\hat{J}_1(\theta) = \frac{1}{M}\sum_{i=1}^{M}L(\theta, x_i)$$$$\hat{J}_2(\theta) = \frac{1}{M}\sum_{i=M+1}^{2M}L(\theta, x_i)$$$$\hat{J}_3(\theta) = \frac{1}{M}\sum_{i=2M+1}^{3M}L(\theta, x_i)$$$$...$$<p>where $M$ is batch size. Note that those are functions of $\theta$ - the parameters of the models, not functions of $x_i$. Each of those functions is an <strong>estimate</strong> of $J(\theta)$.</p>
<p><strong>What if we do not shuffle the data?</strong></p>
<p>If the training set is not shuffled after each epoch which means for every epoch, $\hat{J}_1(\theta)$ get the same parameters (same set of $x_i$) =&gt; They are <strong>fixed functions</strong> (not sure i'm using the right word here). We can prove that, in this case, they are all <strong>biased estimate</strong> of the true $J(\theta)$. Let's pick $\hat{J}_1(\theta)$ for example, its expectation is</p>
$$E_{x_i}[\hat{J}_1(\theta)] = E_{x_i}\left [ \frac{1}{M}\sum_{i=1}^{M}L(\theta, x_i) \right ] \qquad (2)$$<p>If we assume that $x_i$'s are i.i.d which is often the case, then we can have
$$E_{x_i}[\hat{J}_1(\theta)] = \frac{1}{M}\sum_{i=1}^{M} E_{x_i}[L(\theta, x_i)] \qquad (3)$$</p>
<p>Since $x_i$ is fixed, Eq. 3 can be re-written as</p>
$$E_{x_i}[\hat{J}_1(\theta)] = \frac{1}{M}\sum_{i=1}^{M} L(\theta, x_i) \neq \frac{1}{N}\sum_{i=1}^{N}L(\theta, x_i)$$<p>Which makes $\hat{J}_1(\theta)$ a biased estimate of $J(\theta)$. This is not what we want. We want un-biased estimates of $J(\theta)$. So how can we achieve that?</p>
<p><strong>The effect of data shuffling</strong></p>
<p>Let's say instead of fix set of $x_i$ for each $\hat{J}_j$, we draw sample uniformly from the training set for every iteration (every batch). This time, from Eq. 3,  the expectation of $\hat{J}_j$ is</p>
$$E_{x_i}[\hat{J}_1(\theta)] = \frac{1}{M}\sum_{i=1}^{M} E_{x_i}[L(\theta, x_i)]$$$$= \frac{1}{M}\sum_{i=1}^{M} \frac{1}{N} \sum_{k = 1}^{N} L(\theta, x_k) =  \frac{1}{M} M \frac{1}{N} \sum_{k = 1}^{N} L(\theta, x_k) = J(\theta) \qquad (4)$$<p>From Eq. 4 we can see that with $x_i$ being draw uniformly from the training set, $\hat{J}_j$ become an un-biased estimate of $J$. Data shuffling is not randomly drawing $x_i$ uniformly from the training set but it's close and thus has the same effect.</p>
<p><strong>TL;DR</strong>: To answer the questions, data shuffling in mini-batch training produces good gradient updates by un-biasedly estimate the true objective function. If we dont shuffle the data, our model might still converge but it might take longer to train while having higher training and testing error (high bias, since the estimate objective functions are bias).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
</div>
    </div>
    <aside class="postpromonav"><nav><ul class="pager hidden-print">
<li class="previous">
                <a href="../linear-discriminant-analysis/" rel="prev" title="Linear Discriminant Analysis">Previous post</a>
            </li>
        </ul></nav></aside><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js" integrity="sha384-/y1Nn9+QQAipbNQWU65krzJralCnuOasHncUFXGkdwntGeSvQicrYkiUBwsgUqc1" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js" integrity="sha256-ExtbCSBuYA7kq1Pz362ibde9nnsHYPt6JxuxYeZbU+c=" crossorigin="anonymous"></script><script>
                renderMathInElement(document.body,
                    {
                        
delimiters: [
    {left: "$$", right: "$$", display: true},
    {left: "\\[", right: "\\]", display: true},
    {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
    {left: "$", right: "$", display: false},
    {left: "\\(", right: "\\)", display: false}
]

                    }
                );
            </script></article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2019         <a href="mailto:longfet53@gmail.com">Ethan Phan</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
        </footer>
</div>
</div>


            <script src="../../assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script>
</body>
</html>
