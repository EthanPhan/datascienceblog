{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#CD5C5C>Linear Discriminant Analysis(LDA) is a very common technique used for supervised classification problems. Let's dig in to understand what is it, how it works, how we should use it.</font>\n",
    "\n",
    "<a data-flickr-embed=\"true\"  href=\"https://www.flickr.com/photos/15609463@N03/14898932531\" title=\"Reading between the lines\"><img src=\"https://live.staticflickr.com/5574/14898932531_0935b80b98_h.jpg\" width=\"800\" height=\"500\" alt=\"Reading between the lines\"></a><script async src=\"//embedr.flickr.com/assets/client-code.js\" charset=\"utf-8\"></script>\n",
    "\n",
    "<!-- TEASER_END -->\n",
    "\n",
    "**Table of Contents**\n",
    "* [What is Linear Discriminant Analysis ?](#what_is_lda)\n",
    "* [How does Linear Discriminant Analysis Work ?](#howitwork)\n",
    "    * [Fisher's linear discriminant](#fisher_linear_discriminant)\n",
    "    * [Multiple Discriminant Analysis (MDA)](#multiclass_lda)\n",
    "* [Classification with Linear Discriminant Analysis](#classification_with_lda)\n",
    "\n",
    "## <font color=#F08080>What is Linear Discriminant Analysis ?<a class=\"anchor\" id=\"what_is_lda\"></a></font>\n",
    "\n",
    "[Wikipedia: ](https://en.wikipedia.org/wiki/Linear_discriminant_analysis)**Linear discriminant analysis (LDA)**, **normal discriminant analysis (NDA)**, or **discriminant function analysis** is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.\n",
    "\n",
    "The main goal of dimensionality reduction is to remove redundant or dependent features by transforming features to a lower dimension space. So, what is the difference between LDA and PCA? LDA is a supervised learning technique while PCA is usupervised.\n",
    "\n",
    "**The objective of LDA is to perform dimensionality reduction while preserving as much of the class disciminity information as possible.**\n",
    "\n",
    "## <font color=#F08080>How does Linear Discriminant Analysis Work ?<a class=\"anchor\" id=\"howitwork\"></a></font>\n",
    "\n",
    "### Fisher's linear discriminant (2 classes) <a class=\"anchor\" id=\"fisher_linear_discriminant\"></a>\n",
    "\n",
    "Assume we have a set of D-dimensional sample $\\{x_1, x_2, ... , x_N \\}$ $N_1$ of which belong to class $c_1$ and $N_2$ of which belong to class  $c_2$. We seek to obtain a scalar $y$ by projecting samples $x$ onto a line\n",
    "\n",
    "$$ y = w^Tx \\qquad (1)$$\n",
    "\n",
    "We would like to choose the line that maximize the separability of the scalars. **To do that, we need to define a metric to measure the separation of the projections.**\n",
    "\n",
    "Suppose two classes of observations have means $\\vec{\\mu}_1$, $\\vec{\\mu}_2$ and covariances $\\Sigma _{1}$, $\\Sigma _{2}$. Then mean and variance of the projections of the two class are $w^T\\vec{\\mu}_i$ and $w^T \\Sigma _{i} w$ for i = 0, 1.\n",
    "\n",
    "We could choose the distance between the means of the classes as our objective function\n",
    "\n",
    "$$ J(w) = |w^T\\vec{\\mu}_1 - w^T\\vec{\\mu}_2| = |w^T(\\vec{\\mu}_1 - \\vec{\\mu}_2)| \\qquad (2)$$\n",
    "\n",
    "However, distance between the projected means might not be a good measure as you can see here \n",
    "\n",
    "![](https://i.imgur.com/Rq03xZQ.png)\n",
    "\n",
    "Fisher defined the separation between these two distributions to be the ratio of the variance between the classes to the variance within the classes:\n",
    "\n",
    "$$ J(w) = \\frac{\\sigma_{between}}{\\sigma_{within}} = \\frac{(w^T(\\vec{\\mu}_1 - \\vec{\\mu}_2))^2}{w^T \\Sigma _1 w + w^T \\Sigma _2 w} = \\frac{(w^T(\\vec{\\mu}_1 - \\vec{\\mu}_2))^2}{w^T (\\Sigma _1 + \\Sigma _2) w} = \\frac{w^T S_B w}{w^TS_W w} \\qquad (3) $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ S_B = (\\vec{\\mu}_1 - \\vec{\\mu}_2)(\\vec{\\mu}_1 - \\vec{\\mu}_2)^T $$\n",
    "\n",
    "$$ S_W = \\Sigma _1 + \\Sigma _2 $$\n",
    "\n",
    "In some sense, this measure is the signal to noise ratio of class labeling. To find the optimum, we can set the derivative of J and set it to zero\n",
    "\n",
    "$$ \\frac{dJ(w)}{dw} = 0 \\qquad (4)$$\n",
    "\n",
    "$$ \\frac{d[(w^TS_B w)](w^T S_W w) - d[w^T S_W w] (w^T S_B w)}{(w^T S_W w)^2} = 0 \\qquad (5)$$\n",
    "\n",
    "$$ d[(w^TS_B w)] (w^T S_W w)  - d[w^T S_W w](w^T S_B w) = 0 \\qquad (6)$$\n",
    "\n",
    "$$ 2 S_B w(w^T S_W w) - 2S_W w(w^T S_B w) = 0 \\qquad (7)$$\n",
    "\n",
    "Since both $w^T S_W w$ and $w^T S_B w$ are scalars\n",
    "$$ w^T S_W w (S_B w) - w^T S_B w (S_W w) = 0 \\qquad (8)$$\n",
    "\n",
    "$$ \\frac{w^T S_W w (S_B w)}{w^T S_W w} - \\frac{w^T S_B w (S_W w)}{w^T S_W w} = 0 \\qquad (9) $$\n",
    "\n",
    "$$ S_B w = \\lambda S_W w \\qquad (10)$$\n",
    "\n",
    "where $\\lambda = \\frac{w^T S_B w }{w^T S_W w}$ is a scalar depends on $w$\n",
    "If $S_W$ has full rank, Eq. 10 can be written as\n",
    "\n",
    "$$ S^{-1}_W S_B w = \\lambda w \\qquad (11) $$\n",
    "\n",
    "Eq. 11 is a standard eigenvalue problem. \n",
    "\n",
    "Note that $S_B x$ for any vector $x$, points in the same direction as $\\vec{\\mu}_1 - \\vec{\\mu}_2$\n",
    "\n",
    "$$ S_B x = (\\vec{\\mu}_1 - \\vec{\\mu}_2) (\\vec{\\mu}_1 - \\vec{\\mu}_2)^T x =  (\\vec{\\mu}_1 - \\vec{\\mu}_2) \\alpha \\qquad (12)$$\n",
    "\n",
    "Thus Eq. 11 can be solve with \n",
    "\n",
    "$$ \\boxed{w = S^{-1}_W (\\vec{\\mu}_1 - \\vec{\\mu}_2)} \\qquad (13) $$\n",
    "\n",
    "$$ S^{-1}_W S_B w = S^{-1}_W S_B [S^{-1}_W (\\vec{\\mu}_1 - \\vec{\\mu}_2)] = S^{-1}_W \\alpha (\\vec{\\mu}_1 - \\vec{\\mu}_2) = \\alpha [S^{-1}_W (\\vec{\\mu}_1 - \\vec{\\mu}_2)] = \\alpha w \\qquad (14)$$\n",
    "\n",
    "We can easily prove that $\\alpha = \\lambda = \\frac{w^T S_B w }{w^T S_W w}$ Do note that the roles of $\\vec{\\mu}_1$ and $\\vec{\\mu}_2$ are interchangeable.\n",
    "\n",
    "The Eq. 13 is often known as **Fisher's Linear Discriminant** (1936), although it is not a dicriminant but rather a specific choice of direction for the projection od data down to one dimension.\n",
    "\n",
    "Be sure to note that the vector $w$ is the normal to the discriminant hyperplane. As an example, in a two dimensional problem, the line that best divides the two groups is perpendicular to $w$.\n",
    "\n",
    "### Multiple Discriminant Analysis (MDA) <a class=\"anchor\" id=\"multiclass_lda\"></a>\n",
    "\n",
    "In the case where there are more than two classes, the analysis used in the derivation of the Fisher discriminant can be extended to find a subspace which appears to contain all of the class variability.\n",
    "\n",
    "We seek to obtain projection of sample to a linear subspace: $ y = V^T x$. $V$ is call **projection matrix**.\n",
    "\n",
    "Let $n_i$ be the number of samples of class $C_i$; $\\mu_i$ be the sample mean of class $C_i$; and $\\mu$ be the total mean of all samples\n",
    "\n",
    "$$ \\mu_i = \\frac{1}{n_i} \\sum_{x \\in C_i} x \\qquad \\mu = \\frac{1}{n} \\sum_{x_i} x_i $$\n",
    "\n",
    "The scatter within class variability may be defined by the total sum of covariance of each class\n",
    "\n",
    "$$ S_W = \\sum_{i=1}^{c}S_i = \\sum_{i=1}^{c} \\sum_{x_k \\in C_i} (x_k - \\mu_i)(x_k - \\mu_i)^t \\qquad (15)$$\n",
    "\n",
    "The scatter between class variability may be defined by the sample covariance of the class means\n",
    "\n",
    "$$ S_B = \\sum_{i=1}^{c} n_i (\\mu_i - \\mu)(\\mu_i - \\mu)^T \\qquad(16) $$\n",
    "\n",
    "The class seperation for a projection $V$ in this case will be given by\n",
    "\n",
    "$$ J(V) = \\frac{det(V^T \\Sigma_b V)}{det(V^T \\Sigma V)} \\qquad (17) $$\n",
    "\n",
    "To maximize $J(V)$ we first need to solve the **generalized eigenvalue** problem:\n",
    "\n",
    "$$ S_Bv = \\lambda S_W v \\qquad (18)$$\n",
    "\n",
    "Note that matrix $S_B$ is of rank $c-1$ at most, Eq. 18 has atmost $c-1$ distinct solution eigenvalues $\\lambda$. Let $v_1, v_2 ..., v_{c-1}$ be the corresponding eigenvectors. The optimal projection matrix V to a subspace of dimension $k$ is given by the eigenvectors corresponding to the largest $k$ eigenvalues.\n",
    "\n",
    "## <font color=#F08080>Classification with Linear Discriminant Analysis<a class=\"anchor\" id=\"classification_with_lda\"></a></font>\n",
    "\n",
    "So far, LDA has been described as a mean of dimensionality reduction to reduce computational cost and avoid over-fitting. After applying this dimensionality reduction, classification can be performed using Bayes' rule approach. This section will be updated in the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nikola": {
   "category": "",
   "date": "2019-07-08 22:05:59 UTC+08:00",
   "description": "",
   "link": "",
   "slug": "linear-discriminant-analysis",
   "tags": "",
   "title": "Linear Discriminant Analysis",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
